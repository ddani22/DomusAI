"""
ü§ñ DomusAI - Sistema de Scheduling Autom√°tico 24/7

Este m√≥dulo implementa el scheduler que ejecuta tareas peri√≥dicas de forma autom√°tica:
- Detecci√≥n de anomal√≠as cada hora
- Re-entrenamiento de modelos cada 7 d√≠as
- Generaci√≥n de reportes diarios, semanales y mensuales
- Env√≠o autom√°tico de emails

Caracter√≠sticas:
- Ejecuci√≥n 24/7 en segundo plano
- Configuraci√≥n mediante YAML
- Logging exhaustivo
- Error handling robusto
- Reintentos autom√°ticos

Uso:
    python scripts/auto_training_scheduler.py

Para detener:
    Ctrl+C (KeyboardInterrupt)
"""

import sys
import os
from pathlib import Path

# Agregar directorio ra√≠z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

import logging
import time
from datetime import datetime, timedelta
from typing import Dict, Any, Optional
import signal

from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR

# DomusAI imports
from src.auto_trainer import AutoTrainer
from src.anomalies import AnomalyDetector
from src.database import get_db_reader
from src.reporting import ReportGenerator
import pandas as pd
import json
import joblib
from pathlib import Path
import yaml
import traceback
from functools import wraps
from typing import List, Callable


# ============================================================================
# CONFIGURACI√ìN DE LOGGING
# ============================================================================

def retry_with_backoff(max_retries: int = 3, delays: Optional[List[int]] = None):
    """
    Decorador para reintentar funciones con backoff exponencial
    
    Args:
        max_retries: N√∫mero m√°ximo de reintentos
        delays: Lista de delays en segundos (ej: [60, 300, 900])
    
    Example:
        @retry_with_backoff(max_retries=3, delays=[60, 300, 900])
        def my_function():
            # C√≥digo que puede fallar
            pass
    """
    if delays is None:
        delays = [60, 300, 900]  # 1 min, 5 min, 15 min
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception: Optional[Exception] = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt < max_retries:
                        delay = delays[min(attempt, len(delays) - 1)]  # type: ignore
                        logger = logging.getLogger('Scheduler')
                        logger.warning(f"   ‚ö†Ô∏è Intento {attempt + 1}/{max_retries + 1} fall√≥: {e}")
                        logger.info(f"   ‚è≥ Esperando {delay} segundos antes de reintentar...")
                        time.sleep(delay)
                    else:
                        logger = logging.getLogger('Scheduler')
                        logger.error(f"   ‚ùå Todos los reintentos fallaron ({max_retries + 1} intentos)")
                        raise last_exception
            
            # Este c√≥digo nunca se ejecuta, pero satisface el type checker
            if last_exception:
                raise last_exception
        
        return wrapper
    return decorator


def load_config_from_yaml(config_path: str = 'config/scheduler_config.yaml') -> Dict[str, Any]:
    """
    Cargar configuraci√≥n desde archivo YAML
    
    Permite overrides con variables de entorno:
    - DOMUSAI_TIMEZONE
    - DOMUSAI_ANOMALY_ENABLED
    - DOMUSAI_ANOMALY_INTERVAL
    - DOMUSAI_RETRAINING_ENABLED
    - DOMUSAI_RETRAINING_MIN_DAYS
    - DOMUSAI_EMAIL_ENABLED
    
    Args:
        config_path: Ruta al archivo YAML de configuraci√≥n
        
    Returns:
        Dict con configuraci√≥n completa
    """
    # Cargar configuraci√≥n base desde YAML
    config_file = Path(config_path)
    
    if not config_file.exists():
        print(f"‚ö†Ô∏è Archivo de configuraci√≥n no encontrado: {config_path}")
        print("   Usando configuraci√≥n por defecto...")
        return {}
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"‚úÖ Configuraci√≥n cargada desde: {config_path}")
        
        # Aplicar overrides desde variables de entorno
        env_overrides = {
            'DOMUSAI_TIMEZONE': ('general', 'timezone'),
            'DOMUSAI_ANOMALY_ENABLED': ('jobs', 'anomaly_detection', 'enabled'),
            'DOMUSAI_ANOMALY_INTERVAL': ('jobs', 'anomaly_detection', 'interval_minutes'),
            'DOMUSAI_RETRAINING_ENABLED': ('jobs', 'model_retraining', 'enabled'),
            'DOMUSAI_RETRAINING_MIN_DAYS': ('jobs', 'model_retraining', 'min_days_between'),
            'DOMUSAI_EMAIL_ENABLED': ('notifications', 'enabled'),
        }
        
        for env_var, config_path_tuple in env_overrides.items():
            env_value = os.getenv(env_var)
            if env_value is not None:
                # Navegar al path correcto en el dict
                current = config
                for key in config_path_tuple[:-1]:
                    if key not in current:
                        current[key] = {}
                    current = current[key]
                
                # Convertir tipo si es necesario
                last_key = config_path_tuple[-1]
                if env_value.lower() in ['true', 'false']:
                    current[last_key] = env_value.lower() == 'true'
                elif env_value.isdigit():
                    current[last_key] = int(env_value)
                else:
                    current[last_key] = env_value
                
                print(f"   ‚öôÔ∏è Override desde {env_var}: {current[last_key]}")
        
        return config
        
    except yaml.YAMLError as e:
        print(f"‚ùå Error al parsear YAML: {e}")
        print("   Usando configuraci√≥n por defecto...")
        return {}
    except Exception as e:
        print(f"‚ùå Error inesperado al cargar configuraci√≥n: {e}")
        print("   Usando configuraci√≥n por defecto...")
        return {}


def setup_scheduler_logging():
    """Configurar sistema de logging para el scheduler"""
    # Crear directorio de logs si no existe
    os.makedirs('logs', exist_ok=True)
    
    # Configurar formato
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    
    # Logger principal del scheduler
    logger = logging.getLogger('Scheduler')
    logger.setLevel(logging.INFO)
    
    # Handler para archivo
    file_handler = logging.FileHandler('logs/scheduler.log', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Handler para consola
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))
    
    # Agregar handlers
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger


# ============================================================================
# CLASE PRINCIPAL: SchedulerManager
# ============================================================================

class SchedulerManager:
    """
    ü§ñ Gestor del Sistema de Scheduling 24/7
    
    Responsabilidades:
    1. Inicializar APScheduler
    2. Configurar jobs programados
    3. Ejecutar tareas en horarios definidos
    4. Manejar errores y reintentos
    5. Logging exhaustivo
    6. Shutdown graceful
    
    Jobs implementados:
    - hourly_anomaly_detection: Detectar anomal√≠as cada hora
    - daily_retraining_check: Verificar si re-entrenar modelos (3 AM)
    - generate_daily_report: Reporte diario (8 AM)
    - generate_weekly_report: Reporte semanal (Lunes 9 AM)
    - generate_monthly_report: Reporte mensual (D√≠a 1, 10 AM)
    
    Example:
        >>> manager = SchedulerManager()
        >>> manager.start()
        >>> # Scheduler corre 24/7 hasta Ctrl+C
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Inicializar SchedulerManager
        
        Args:
            config: Diccionario de configuraci√≥n (opcional)
                   Si no se provee, carga desde config/scheduler_config.yaml
        """
        self.logger = setup_scheduler_logging()
        
        # Cargar configuraci√≥n desde YAML si no se provee
        if config is None:
            self.logger.info("üìÑ Cargando configuraci√≥n desde YAML...")
            config = load_config_from_yaml()
        
        # Si load_config_from_yaml fall√≥ o retorn√≥ vac√≠o, usar defaults
        self.config = config if config else self._get_default_config()
        
        self.scheduler: Optional[BackgroundScheduler] = None
        self.db_reader = None
        
        # Contadores de estad√≠sticas
        self.stats = {
            'jobs_executed': 0,
            'jobs_failed': 0,
            'anomalies_detected': 0,
            'models_retrained': 0,
            'reports_generated': 0,
            'emails_sent': 0
        }
        
        self.logger.info("=" * 70)
        self.logger.info("ü§ñ SchedulerManager DomusAI inicializado")
        self.logger.info("=" * 70)
    
    def _get_default_config(self) -> Dict[str, Any]:
        """
        Obtener configuraci√≥n por defecto
        
        Returns:
            Dict con configuraci√≥n por defecto del scheduler
        """
        return {
            'timezone': 'Europe/Madrid',  # Ajustar a tu timezone
            'jobs': {
                'anomaly_detection': {
                    'enabled': True,
                    'interval_minutes': 60  # Cada hora
                },
                'model_retraining': {
                    'enabled': True,
                    'cron': '0 3 * * *',  # 3:00 AM diario
                    'min_days_between': 7
                },
                'daily_report': {
                    'enabled': True,
                    'cron': '0 8 * * *',  # 8:00 AM diario
                },
                'weekly_report': {
                    'enabled': True,
                    'cron': '0 9 * * 1',  # 9:00 AM cada lunes
                },
                'monthly_report': {
                    'enabled': True,
                    'cron': '0 10 1 * *',  # 10:00 AM d√≠a 1 de mes
                }
            },
            'notifications': {
                'enabled': True,
                'email_on_error': True,
                'email_on_success': False
            }
        }
    
    def initialize_scheduler(self):
        """
        Inicializar APScheduler con configuraci√≥n
        """
        self.logger.info("üîß Inicializando APScheduler...")
        
        # Obtener timezone desde config (YAML o default)
        timezone = self.config.get('general', {}).get('timezone', 'Europe/Madrid')
        
        # Crear scheduler con BackgroundScheduler (no-blocking)
        self.scheduler = BackgroundScheduler(timezone=timezone)
        
        # Agregar listeners para eventos
        self.scheduler.add_listener(
            self._on_job_executed,
            EVENT_JOB_EXECUTED
        )
        
        self.scheduler.add_listener(
            self._on_job_error,
            EVENT_JOB_ERROR
        )
        
        self.logger.info(f"‚úÖ APScheduler inicializado (timezone: {timezone})")
    
    def add_jobs(self):
        """
        Agregar todos los jobs programados al scheduler
        """
        assert self.scheduler is not None, "Scheduler debe estar inicializado antes de agregar jobs"
        
        self.logger.info("üìã Agregando jobs al scheduler...")
        
        jobs_config = self.config.get('jobs', {})
        
        # Job 1: Detecci√≥n de anomal√≠as (cada hora)
        if jobs_config.get('anomaly_detection', {}).get('enabled', True):
            interval = jobs_config['anomaly_detection'].get('interval_minutes', 60)
            self.scheduler.add_job(
                func=self.hourly_anomaly_detection,
                trigger=IntervalTrigger(minutes=interval),
                id='anomaly_detection',
                name='Detecci√≥n de Anomal√≠as',
                max_instances=1,  # Solo 1 instancia corriendo a la vez
                replace_existing=True
            )
            self.logger.info(f"   ‚úÖ Job agregado: Detecci√≥n de Anomal√≠as (cada {interval} min)")
        
        # Job 2: Re-entrenamiento de modelos (3 AM diario)
        if jobs_config.get('model_retraining', {}).get('enabled', True):
            cron = jobs_config['model_retraining'].get('cron', '0 3 * * *')
            self.scheduler.add_job(
                func=self.daily_retraining_check,
                trigger=CronTrigger.from_crontab(cron),
                id='model_retraining',
                name='Re-entrenamiento de Modelos',
                max_instances=1,
                replace_existing=True
            )
            self.logger.info(f"   ‚úÖ Job agregado: Re-entrenamiento de Modelos ({cron})")
        
        # Job 3: Reporte diario (8 AM)
        if jobs_config.get('daily_report', {}).get('enabled', True):
            cron = jobs_config['daily_report'].get('cron', '0 8 * * *')
            self.scheduler.add_job(
                func=self.generate_daily_report,
                trigger=CronTrigger.from_crontab(cron),
                id='daily_report',
                name='Reporte Diario',
                max_instances=1,
                replace_existing=True
            )
            self.logger.info(f"   ‚úÖ Job agregado: Reporte Diario ({cron})")
        
        # Job 4: Reporte semanal (Lunes 9 AM)
        if jobs_config.get('weekly_report', {}).get('enabled', True):
            cron = jobs_config['weekly_report'].get('cron', '0 9 * * 1')
            self.scheduler.add_job(
                func=self.generate_weekly_report,
                trigger=CronTrigger.from_crontab(cron),
                id='weekly_report',
                name='Reporte Semanal',
                max_instances=1,
                replace_existing=True
            )
            self.logger.info(f"   ‚úÖ Job agregado: Reporte Semanal ({cron})")
        
        # Job 5: Reporte mensual (D√≠a 1, 10 AM)
        if jobs_config.get('monthly_report', {}).get('enabled', True):
            cron = jobs_config['monthly_report'].get('cron', '0 10 1 * *')
            self.scheduler.add_job(
                func=self.generate_monthly_report,
                trigger=CronTrigger.from_crontab(cron),
                id='monthly_report',
                name='Reporte Mensual',
                max_instances=1,
                replace_existing=True
            )
            self.logger.info(f"   ‚úÖ Job agregado: Reporte Mensual ({cron})")
        
        self.logger.info(f"‚úÖ Total de jobs agregados: {len(self.scheduler.get_jobs())}")
    
    # ========================================================================
    # EVENT LISTENERS
    # ========================================================================
    
    def _on_job_executed(self, event):
        """Callback cuando un job se ejecuta exitosamente"""
        self.stats['jobs_executed'] += 1
        self.logger.info(f"‚úÖ Job ejecutado: {event.job_id}")
    
    def _on_job_error(self, event):
        """Callback cuando un job falla"""
        self.stats['jobs_failed'] += 1
        self.logger.error(f"‚ùå Job fall√≥: {event.job_id} - {event.exception}")
    
    # ========================================================================
    # JOB FUNCTIONS (Implementaciones b√°sicas - se completar√°n en subtareas)
    # ========================================================================
    
    def hourly_anomaly_detection(self):
        """
        üïê Job: Detecci√≥n de anomal√≠as cada hora
        
        Flujo:
        1. Obtener √∫ltimas 60 lecturas de Railway (√∫ltima hora)
        2. Cargar modelo Isolation Forest
        3. Detectar anomal√≠as
        4. Si hay anomal√≠as ‚Üí marcar en Railway
        5. Calcular severidad y enviar email si necesario
        6. Actualizar estad√≠sticas
        """
        self.logger.info("üïê [HOURLY] Ejecutando detecci√≥n de anomal√≠as...")
        start_time = time.time()
        
        try:
            # PASO 1: Obtener datos recientes de Railway
            self.logger.info("   üìä Obteniendo √∫ltimas 60 lecturas de Railway...")
            
            if self.db_reader is None:
                self.db_reader = get_db_reader()
            
            # Obtener √∫ltima hora de datos
            df = self.db_reader.get_recent_readings(hours=1)
            
            if df is None or df.empty:
                self.logger.warning("   ‚ö†Ô∏è No hay datos recientes en Railway")
                return
            
            num_readings = len(df)
            self.logger.info(f"   ‚úÖ {num_readings} lecturas obtenidas")
            
            # Validar datos m√≠nimos
            if num_readings < 30:
                self.logger.warning(f"   ‚ö†Ô∏è Datos insuficientes ({num_readings} < 30)")
                self.logger.info("   ‚ÑπÔ∏è Se necesitan al menos 30 lecturas para detecci√≥n confiable")
                return
            
            # PASO 2: Cargar modelo de detecci√≥n de anomal√≠as
            self.logger.info("   ü§ñ Cargando modelo de detecci√≥n...")
            
            model_path = Path('models/best_isolation_forest.pkl')
            if not model_path.exists():
                self.logger.error(f"   ‚ùå Modelo no encontrado: {model_path}")
                self.logger.info("   ‚ÑπÔ∏è Ejecutar AutoTrainer primero para generar el modelo")
                return
            
            # Cargar modelo con joblib
            try:
                anomaly_model = joblib.load(model_path)
                self.logger.info(f"   ‚úÖ Modelo cargado: {model_path}")
            except Exception as e:
                self.logger.error(f"   ‚ùå Error al cargar modelo: {e}")
                return
            
            # PASO 3: Detectar anomal√≠as
            self.logger.info("   üîç Detectando anomal√≠as...")
            
            # Preparar features para el modelo
            feature_cols = ['Global_active_power', 'Global_reactive_power', 
                          'Voltage', 'Global_intensity']
            
            # Verificar que tenemos todas las columnas necesarias
            missing_cols = [col for col in feature_cols if col not in df.columns]
            if missing_cols:
                self.logger.error(f"   ‚ùå Columnas faltantes: {missing_cols}")
                return
            
            X = df[feature_cols].copy()
            
            # Manejar valores nulos si existen
            if X.isnull().any().any():
                self.logger.warning("   ‚ö†Ô∏è Datos con valores nulos, rellenando con media...")
                X = X.fillna(X.mean())
            
            # Predecir anomal√≠as (-1 = anomal√≠a, 1 = normal)
            predictions = anomaly_model.predict(X)
            
            # Convertir a booleano (True = anomal√≠a)
            df['is_anomaly'] = predictions == -1
            
            # PASO 4: Contar anomal√≠as
            num_anomalies = df['is_anomaly'].sum()
            
            if num_anomalies == 0:
                self.logger.info("   ‚úÖ Sin anomal√≠as detectadas en la √∫ltima hora")
                duration = time.time() - start_time
                self.logger.info(f"   ‚è±Ô∏è Detecci√≥n completada en {duration:.1f} segundos")
                return
            
            # Hay anomal√≠as - procesar
            self.logger.warning(f"   ‚ö†Ô∏è {num_anomalies} anomal√≠as detectadas:")
            
            # Obtener detalles de las anomal√≠as
            anomalies_df = df[df['is_anomaly'] == True].copy()
            
            for idx, row in anomalies_df.iterrows():
                timestamp = row.name if isinstance(row.name, pd.Timestamp) else row.get('Datetime', 'Unknown')
                power = row.get('Global_active_power', 0)
                self.logger.warning(f"      ‚Ä¢ {timestamp} - {power:.2f} kW")
            
            # PASO 5: Calcular severidad
            avg_power = anomalies_df['Global_active_power'].mean()
            max_power = anomalies_df['Global_active_power'].max()
            
            # Criterios de severidad
            if num_anomalies > 5 or max_power > 8:
                severity = 'HIGH'
                emoji = 'üî¥'
            elif num_anomalies > 2 or max_power > 5:
                severity = 'MEDIUM'
                emoji = 'üü°'
            else:
                severity = 'LOW'
                emoji = 'üü¢'
            
            self.logger.info(f"   {emoji} Severidad: {severity}")
            self.logger.info(f"      Potencia promedio: {avg_power:.2f} kW")
            self.logger.info(f"      Potencia m√°xima: {max_power:.2f} kW")
            
            # PASO 6: Marcar anomal√≠as en Railway (TODO en pr√≥xima versi√≥n)
            # Por ahora solo logeamos, en el futuro se har√° UPDATE a Railway
            self.logger.info("   üìù Marcado de anomal√≠as en Railway: PENDIENTE")
            self.logger.info("   ‚ÑπÔ∏è (Requiere permisos de escritura en Railway)")
            
            # PASO 7: Enviar email si severidad >= MEDIUM (TODO)
            if severity in ['MEDIUM', 'HIGH']:
                self.logger.info(f"   üìß Email de alerta necesario (severidad: {severity})")
                self.logger.info("   ‚ÑπÔ∏è Env√≠o de email: PENDIENTE (implementar en siguiente versi√≥n)")
                # TODO: Implementar env√≠o de email
                # from src.email_sender import send_anomaly_alert
                # send_anomaly_alert(anomaly_summary)
                # self.stats['emails_sent'] += 1
            
            # PASO 8: Actualizar estad√≠sticas
            self.stats['anomalies_detected'] += num_anomalies
            
            duration = time.time() - start_time
            self.logger.info(f"   ‚úÖ Detecci√≥n completada en {duration:.1f} segundos")
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Error en detecci√≥n de anomal√≠as: {e}")
            import traceback
            self.logger.error(f"   Stack trace:\n{traceback.format_exc()}")
            raise
    
    def daily_retraining_check(self):
        """
        üåô Job: Verificar si necesita re-entrenamiento (3 AM)
        
        Flujo:
        1. Leer √∫ltima fecha de entrenamiento
        2. Calcular d√≠as transcurridos
        3. Si >= 7 d√≠as ‚Üí ejecutar AutoTrainer
        4. Verificar resultado y actualizar estad√≠sticas
        5. Enviar notificaci√≥n
        """
        self.logger.info("üåô [DAILY] Verificando necesidad de re-entrenamiento...")
        start_time = time.time()
        
        try:
            # PASO 1: Verificar √∫ltima fecha de entrenamiento
            history_path = Path('logs/metrics_history.json')
            
            if not history_path.exists():
                self.logger.warning("   ‚ö†Ô∏è No hay historial de entrenamiento")
                self.logger.info("   üöÄ Primera ejecuci√≥n - iniciando entrenamiento...")
                should_train = True
                days_since = 999  # Valor alto para forzar entrenamiento
            else:
                try:
                    with open(history_path, 'r', encoding='utf-8') as f:
                        history = json.load(f)
                    
                    if not history:
                        self.logger.warning("   ‚ö†Ô∏è Historial vac√≠o")
                        should_train = True
                        days_since = 999
                    else:
                        # Obtener √∫ltima entrada
                        last_entry = history[-1]
                        last_date_str = last_entry.get('timestamp', '')
                        
                        if not last_date_str:
                            self.logger.warning("   ‚ö†Ô∏è Sin timestamp en historial")
                            should_train = True
                            days_since = 999
                        else:
                            # Parsear fecha
                            last_date = datetime.fromisoformat(last_date_str)
                            days_since = (datetime.now() - last_date).days
                            
                            self.logger.info(f"   üìÖ √öltimo entrenamiento: {last_date.strftime('%Y-%m-%d %H:%M:%S')}")
                            self.logger.info(f"   ‚è±Ô∏è D√≠as transcurridos: {days_since}")
                            
                            # Obtener configuraci√≥n de d√≠as m√≠nimos
                            min_days = self.config['jobs']['model_retraining'].get('min_days_between', 7)
                            
                            if days_since < min_days:
                                self.logger.info(f"   ‚úÖ Modelo reciente, pr√≥ximo entrenamiento en {min_days - days_since} d√≠as")
                                return
                            else:
                                self.logger.info(f"   üöÄ Necesita re-entrenamiento (>= {min_days} d√≠as)")
                                should_train = True
                
                except json.JSONDecodeError as e:
                    self.logger.error(f"   ‚ùå Error al leer historial: {e}")
                    self.logger.info("   üöÄ Continuando con entrenamiento...")
                    should_train = True
                    days_since = 999
            
            if not should_train:
                return
            
            # PASO 2: Verificar datos suficientes en Railway
            self.logger.info("   üìä Verificando datos disponibles en Railway...")
            
            if self.db_reader is None:
                self.db_reader = get_db_reader()
            
            try:
                stats = self.db_reader.get_statistics()
                total_records = stats.get('total_readings', 0)
                self.logger.info(f"   ‚ÑπÔ∏è Total de registros: {total_records:,}")
                
                # M√≠nimo 30 d√≠as de datos (30 d√≠as √ó 1440 lecturas/d√≠a = 43,200)
                MIN_RECORDS = 43200
                
                if total_records < MIN_RECORDS:
                    self.logger.warning(f"   ‚ö†Ô∏è Datos insuficientes: {total_records:,} < {MIN_RECORDS:,}")
                    self.logger.info("   ‚ÑπÔ∏è Se necesitan al menos 30 d√≠as de datos")
                    self.logger.info("   ‚ÑπÔ∏è Esperando m√°s datos antes de entrenar")
                    return
                
                self.logger.info(f"   ‚úÖ Datos suficientes: {total_records:,} registros")
                
                # Calcular d√≠as aproximados de datos
                days_of_data = total_records / 1440
                self.logger.info(f"   ‚ÑπÔ∏è Aproximadamente {days_of_data:.1f} d√≠as de datos")
                
            except Exception as e:
                self.logger.error(f"   ‚ùå Error al verificar datos: {e}")
                self.logger.info("   ‚ÑπÔ∏è Continuando con entrenamiento de todos modos...")
            
            # PASO 3: Ejecutar AutoTrainer pipeline
            self.logger.info("=" * 70)
            self.logger.info("   ü§ñ INICIANDO PIPELINE DE RE-ENTRENAMIENTO")
            self.logger.info("=" * 70)
            
            try:
                # Inicializar AutoTrainer
                trainer = AutoTrainer(
                    data_source='railway',
                    training_window_days=90  # Usar √∫ltimos 90 d√≠as
                )
                
                self.logger.info("   üîß AutoTrainer inicializado")
                self.logger.info("   üìä Fuente de datos: Railway MySQL")
                self.logger.info("   üìÖ Ventana de entrenamiento: 90 d√≠as")
                
                # Ejecutar pipeline completo (11 pasos)
                self.logger.info("   üöÄ Ejecutando pipeline completo...")
                result = trainer.run_full_training_pipeline()
                
                # PASO 4: Verificar resultado
                if result.get('success', False):
                    version_id = result.get('version_id', 'unknown')
                    metrics = result.get('metrics', {})
                    comparison = result.get('comparison', {})
                    
                    self.logger.info("=" * 70)
                    self.logger.info("   ‚úÖ RE-ENTRENAMIENTO EXITOSO")
                    self.logger.info("=" * 70)
                    self.logger.info(f"   üÜî Versi√≥n: {version_id}")
                    
                    # M√©tricas del nuevo modelo
                    if metrics:
                        mae = metrics.get('mae', 0)
                        rmse = metrics.get('rmse', 0)
                        r2 = metrics.get('r2', 0)
                        
                        self.logger.info(f"   üìä M√©tricas del nuevo modelo:")
                        self.logger.info(f"      MAE:  {mae:.4f}")
                        self.logger.info(f"      RMSE: {rmse:.4f}")
                        self.logger.info(f"      R¬≤:   {r2:.4f}")
                    
                    # Comparaci√≥n con modelo anterior
                    if comparison:
                        mae_improvement = comparison.get('mae_improvement_pct', 0)
                        decision = comparison.get('decision', 'unknown')
                        
                        self.logger.info(f"   üìà Comparaci√≥n con anterior:")
                        self.logger.info(f"      Mejora MAE: {mae_improvement:+.1f}%")
                        self.logger.info(f"      Decisi√≥n: {decision}")
                    
                    # Actualizar estad√≠sticas
                    self.stats['models_retrained'] += 1
                    
                    duration = time.time() - start_time
                    self.logger.info(f"   ‚è±Ô∏è Tiempo total: {duration:.1f} segundos ({duration/60:.1f} minutos)")
                    
                    # TODO: Enviar email de √©xito
                    self.logger.info("   üìß Email de confirmaci√≥n: PENDIENTE")
                    # self.stats['emails_sent'] += 1
                    
                else:
                    error = result.get('error', 'Error desconocido')
                    self.logger.error("=" * 70)
                    self.logger.error("   ‚ùå RE-ENTRENAMIENTO FALL√ì")
                    self.logger.error("=" * 70)
                    self.logger.error(f"   Error: {error}")
                    
                    # TODO: Enviar email de fallo
                    self.logger.error("   üìß Email de alerta cr√≠tica: PENDIENTE")
                    
                    duration = time.time() - start_time
                    self.logger.error(f"   ‚è±Ô∏è Tiempo transcurrido: {duration:.1f} segundos")
            
            except Exception as e:
                self.logger.error("=" * 70)
                self.logger.error("   ‚ùå EXCEPCI√ìN DURANTE RE-ENTRENAMIENTO")
                self.logger.error("=" * 70)
                self.logger.error(f"   Error: {e}")
                import traceback
                self.logger.error(f"   Stack trace:\n{traceback.format_exc()}")
                
                # TODO: Enviar email de fallo cr√≠tico
                self.logger.error("   üìß Email de alerta cr√≠tica: PENDIENTE")
                
                raise
            
            # PASO 5: Cleanup (liberar memoria)
            self.logger.info("   üßπ Liberando memoria...")
            import gc
            gc.collect()
            self.logger.info("   ‚úÖ Cleanup completado")
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Error en check de re-entrenamiento: {e}")
            import traceback
            self.logger.error(f"   Stack trace:\n{traceback.format_exc()}")
            raise
    
    def generate_daily_report(self):
        """
        ‚òÄÔ∏è Job: Generar reporte diario (8 AM)
        
        Genera reporte HTML de las √∫ltimas 24 horas con Railway MySQL
        """
        self.logger.info("‚òÄÔ∏è [DAILY] Generando reporte diario...")
        start_time = time.time()
        
        try:
            # PASO 1: Conectar a Railway
            self.logger.info("   üì° Conectando a Railway MySQL...")
            db_reader = get_db_reader()
            
            # PASO 2: Generar reporte
            self.logger.info("   üìä Generando reporte HTML...")
            generator = ReportGenerator()
            
            result = generator.generate_daily_report(
                db_reader=db_reader,
                predictions=None,  # TODO: Integrar con predictor en siguiente versi√≥n
                anomalies=None     # TODO: Integrar con detector en siguiente versi√≥n
            )
            
            # PASO 3: Validar resultado
            if result.get('status') == 'success':
                self.logger.info(f"   ‚úÖ Reporte diario generado: {result['html_path']}")
                self.logger.info(f"      Registros: {result['summary']['total_records']:,}")
                self.logger.info(f"      Consumo promedio: {result['summary']['avg_consumption']:.3f} kW")
                self.logger.info(f"      Fuente: {result['data_source']}")
                
                # Actualizar estad√≠sticas
                self.stats['reports_generated'] += 1
                
                # TODO: Enviar email con reporte adjunto
                self.logger.info("   üìß Email de reporte: PENDIENTE (siguiente versi√≥n)")
                
            else:
                self.logger.error(f"   ‚ùå Error: {result.get('error')}")
                self.logger.warning("   ‚ÑπÔ∏è Puede ser falta de datos recientes en Railway")
            
            duration = time.time() - start_time
            self.logger.info(f"   ‚è±Ô∏è Completado en {duration:.1f} segundos")
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Error generando reporte diario: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise
    
    def generate_weekly_report(self):
        """
        üìÖ Job: Generar reporte semanal (Lunes 9 AM)
        
        Genera reporte HTML de los √∫ltimos 7 d√≠as con Railway MySQL
        """
        self.logger.info("üìÖ [WEEKLY] Generando reporte semanal...")
        start_time = time.time()
        
        try:
            # PASO 1: Conectar a Railway
            self.logger.info("   üì° Conectando a Railway MySQL...")
            db_reader = get_db_reader()
            
            # PASO 2: Generar reporte
            self.logger.info("   üìä Generando reporte HTML...")
            generator = ReportGenerator()
            
            result = generator.generate_weekly_report(
                db_reader=db_reader,
                predictions=None,  # TODO: Integrar con predictor en siguiente versi√≥n
                anomalies=None     # TODO: Integrar con detector en siguiente versi√≥n
            )
            
            # PASO 3: Validar resultado
            if result.get('status') == 'success':
                self.logger.info(f"   ‚úÖ Reporte semanal generado: {result['html_path']}")
                self.logger.info(f"      Registros: {result['summary']['total_records']:,}")
                self.logger.info(f"      Consumo diario promedio: {result['summary']['avg_daily_kwh']:.2f} kWh")
                self.logger.info(f"      Total semanal: {result['summary']['total_weekly_kwh']:.2f} kWh")
                self.logger.info(f"      Fuente: {result['data_source']}")
                
                # Actualizar estad√≠sticas
                self.stats['reports_generated'] += 1
                
                # TODO: Enviar email con reporte adjunto
                self.logger.info("   üìß Email de reporte: PENDIENTE (siguiente versi√≥n)")
                
            else:
                self.logger.error(f"   ‚ùå Error: {result.get('error')}")
                self.logger.warning("   ‚ÑπÔ∏è Puede ser falta de datos recientes en Railway")
            
            duration = time.time() - start_time
            self.logger.info(f"   ‚è±Ô∏è Completado en {duration:.1f} segundos")
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Error generando reporte semanal: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise
    
    def generate_monthly_report(self):
        """
        üìä Job: Generar reporte mensual (D√≠a 1, 10 AM)
        
        Genera reporte HTML/PDF del mes anterior con Railway MySQL
        """
        self.logger.info("üìä [MONTHLY] Generando reporte mensual...")
        start_time = time.time()
        
        try:
            # PASO 1: Determinar mes a reportar (mes anterior)
            now = datetime.now()
            if now.month == 1:
                report_month = 12
                report_year = now.year - 1
            else:
                report_month = now.month - 1
                report_year = now.year
            
            self.logger.info(f"   üìÖ Reporte para: {report_month}/{report_year}")
            
            # PASO 2: Conectar a Railway
            self.logger.info("   üì° Conectando a Railway MySQL...")
            db_reader = get_db_reader()
            
            # PASO 3: Generar reporte
            self.logger.info("   üìä Generando reporte HTML...")
            generator = ReportGenerator()
            
            result = generator.generate_monthly_report(
                db_reader=db_reader,
                predictions=None,  # TODO: Integrar con predictor en siguiente versi√≥n
                anomalies=None,    # TODO: Integrar con detector en siguiente versi√≥n
                month=report_month,
                year=report_year
            )
            
            # PASO 4: Validar resultado
            if result.get('status') == 'success':
                self.logger.info(f"   ‚úÖ Reporte mensual generado: {result['html_path']}")
                summary = result.get('summary', {})
                self.logger.info(f"      Consumo total: {summary.get('total_consumption', 0):.2f} kWh")
                self.logger.info(f"      Consumo diario promedio: {summary.get('daily_avg', 0):.3f} kW")
                self.logger.info(f"      Cambio vs mes anterior: {summary.get('change_pct', 0):+.1f}%")
                self.logger.info(f"      Score de eficiencia: {summary.get('efficiency_score', 0)}/100")
                self.logger.info(f"      Fuente: {result.get('data_source')}")
                
                # Actualizar estad√≠sticas
                self.stats['reports_generated'] += 1
                
                # TODO: Generar PDF y enviar email
                self.logger.info("   üìß Email con PDF: PENDIENTE (siguiente versi√≥n)")
                
            else:
                self.logger.error(f"   ‚ùå Error: {result.get('error')}")
                self.logger.warning(f"   ‚ÑπÔ∏è Puede ser falta de datos para {report_month}/{report_year}")
            
            duration = time.time() - start_time
            self.logger.info(f"   ‚è±Ô∏è Completado en {duration:.1f} segundos")
            
        except Exception as e:
            self.logger.error(f"   ‚ùå Error generando reporte mensual: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise
    
    # ========================================================================
    # CONTROL DEL SCHEDULER
    # ========================================================================
    
    def start(self):
        """
        Iniciar el scheduler (modo 24/7)
        
        El scheduler corre en segundo plano hasta que se detiene con Ctrl+C
        """
        self.logger.info("=" * 70)
        self.logger.info("üöÄ INICIANDO SCHEDULER DOMUSAI")
        self.logger.info("=" * 70)
        
        # Inicializar scheduler
        self.initialize_scheduler()
        
        # Agregar jobs
        self.add_jobs()
        
        # Mostrar pr√≥ximas ejecuciones
        self._print_next_jobs()
        
        # Iniciar scheduler
        assert self.scheduler is not None, "Scheduler debe estar inicializado antes de iniciar"
        self.scheduler.start()
        self.logger.info("‚úÖ Scheduler iniciado - corriendo 24/7")
        self.logger.info("   Para detener: Ctrl+C")
        self.logger.info("=" * 70)
        
        try:
            # Mantener el programa corriendo
            while True:
                time.sleep(60)  # Check cada minuto
                
        except (KeyboardInterrupt, SystemExit):
            self.logger.info("\nüõë Deteniendo scheduler...")
            self.shutdown()
    
    def shutdown(self):
        """
        Detener el scheduler de forma graceful
        """
        if self.scheduler and self.scheduler.running:
            self.scheduler.shutdown(wait=True)
            self.logger.info("‚úÖ Scheduler detenido correctamente")
            
            # Mostrar estad√≠sticas finales
            self._print_stats()
    
    def _print_next_jobs(self):
        """Mostrar pr√≥ximas ejecuciones programadas"""
        assert self.scheduler is not None, "Scheduler debe estar inicializado"
        
        self.logger.info("\nüìÖ Pr√≥ximas ejecuciones programadas:")
        
        jobs = self.scheduler.get_jobs()
        for job in jobs:
            # En APScheduler 3.x, next_run_time puede no estar disponible antes de start()
            # Por eso solo mostramos el nombre del job
            self.logger.info(f"   ‚Ä¢ {job.name}")
        
        self.logger.info("")
    
    def _print_stats(self):
        """Mostrar estad√≠sticas de ejecuci√≥n"""
        self.logger.info("\nüìä Estad√≠sticas de ejecuci√≥n:")
        self.logger.info(f"   Jobs ejecutados: {self.stats['jobs_executed']}")
        self.logger.info(f"   Jobs fallidos: {self.stats['jobs_failed']}")
        self.logger.info(f"   Anomal√≠as detectadas: {self.stats['anomalies_detected']}")
        self.logger.info(f"   Modelos re-entrenados: {self.stats['models_retrained']}")
        self.logger.info(f"   Reportes generados: {self.stats['reports_generated']}")
        self.logger.info(f"   Emails enviados: {self.stats['emails_sent']}")


# ============================================================================
# FUNCI√ìN MAIN
# ============================================================================

def main():
    """
    Funci√≥n principal para ejecutar el scheduler
    """
    print("=" * 70)
    print("ü§ñ DomusAI - Sistema de Scheduling Autom√°tico")
    print("=" * 70)
    print()
    
    # Crear y ejecutar scheduler
    manager = SchedulerManager()
    
    # Configurar signal handlers para shutdown graceful
    def signal_handler(signum, frame):
        manager.logger.info(f"\nüõë Se√±al recibida: {signum}")
        manager.shutdown()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Iniciar scheduler
    manager.start()


if __name__ == "__main__":
    main()
